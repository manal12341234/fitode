%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{dplyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Basic ODE model fitting}
\documentclass{article}
\title{Basic ODE fitting}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{hyperref}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\bibliographystyle{chicago}
\date{\today}
\begin{document}
\maketitle

<<opts,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=6,fig.height=4)
@

\tableofcontents

\section{Preliminaries}

Load packages:

<<pkgs,message=FALSE>>=
library(fitode)
library(dplyr)
library(tidyr)
library(ggplot2); theme_set(theme_bw())
library(rbenchmark)
@

\section{Basic fitting}

\subsection{Exponential decay model}

Suppose we have a quantity that is decreasing exponentially.

<<>>==
set.seed(123)
true.m <- 0.5
true.A0 <- 200
true.sd <- 15

exp.data <- data.frame(
    x=rep(0:4, 3),
    y=rnorm(15, true.A0 * exp(-true.m * rep(0:4, 3)), sd=true.sd)
)

plot(exp.data)
@

The true dynamics can be modeled with the following equation:
$$
\frac{dA}{dt} = - m A
$$

We can translate this into a \code{fitode} model as follows:

<<>>==
exp.model <- new("model.ode",
    name = "SI",
    model = list(
        A ~ -m * A
    ),
    observation = list(
        y ~ dnorm(mean=A, sd=sd)
    ),
    initial = list(
        A ~ A0
    ),
    par=c("m", "A0", "sd")
)
@

Then, we can fit the model:

<<>>===
exp.fit <- fitode(
    exp.model,
    exp.data,
    start=c(m=0.5, A0=200, sd=15),
    tcol="x"
)
@

To diagnose the fit, we can use \code{plot} function.
Using the \code{level} argument will plot 95\% confidence intervals of the true trajectory, estimated via delta method.

<<>>==
plot(exp.fit, level=0.95)
curve(true.A0 * exp(-true.m*x), add=TRUE, lty=1, col="green")
legend(
    x="topright",
    legend=c("estimated", "true"),
    col=c("black", "green"),
    lty=1
)
@

To obtain the confidence interval, we can use \code{confint} function.
There are three available methods for obtaining the confidence intervals: \code{delta}, \code{profile} and \code{wmvrnorm}.
Due to computation speed, default option is \code{delta}.
We will get into the details later.
For now,

<<>>==
confint(exp.fit)
@

Note that in this particular example, we can use \code{glm} function to fit the model as well.
We can compare the results.

<<>>==
glm.fit <- glm(y~x,
    family=gaussian(link="log"),
    data=exp.data,
    start = c(intercept=log(200), x=-0.5))

glm.pred <- predict(glm.fit, data.frame(x=0:4), se.fit=TRUE, type="response")

glm.data <- data.frame(
    x=0:4,
    estimate=glm.pred$fit,
    lwr=glm.pred$fit-1.96 * glm.pred$se.fit,
    upr=glm.pred$fit+1.96 * glm.pred$se.fit
)

plot(exp.fit, level=0.95)
lines(glm.data$x, glm.data$estimate, col=2)
lines(glm.data$x, glm.data$lwr, col=2)
lines(glm.data$x, glm.data$upr, col=2)
legend(
    x="topright",
    legend=c("estimated (fitode)", "estimated (glm)"),
    col=c("black", "red"),
    lty=1
)
@

Predicted trajectories and their confidence intervals are essentially indistinguishable.

\subsection{Chemical reaction - multiple state fitting}

Now, consider the following chemical reaction:
$$
A \to 3 B
$$
Then, we can write the governing differential equation as follows:
$$
\begin{aligned}
\frac{dA}{dt} &= - k A\\
\frac{dB}{dt} &= 3 k B
\end{aligned}
$$
Suppose we have measured both quantities and have data:
<<echo=FALSE>>==
reaction_model <- new("model.ode",
    name = "reaction",
    model = list(
        A ~ - k * A,
        B ~ 3 * k * A
    ),
    observation = list(
        y1 ~ dnorm(mean=A, sd=sd1),
        y2 ~ dnorm(mean=B, sd=sd2)
    ),
    initial = list(
        A~A0,
        B~B0
    ),
    par=c("k", "A0", "B0", "sd1", "sd2")
)

true.k <- 0.1
true.A0 <- 300
true.B0 <- 100
true.sd1 <- 10
true.sd2 <- 10

times <- 1:20

reaction_run <- ode.solve(reaction_model, times, c(k=true.k, A0=true.A0, B0=true.B0, sd1=true.sd1, sd2=true.sd2))
reaction_data <- data.frame(
    times=times,
    y1=rnorm(20, reaction_run@solution$A, sd=true.sd1),
    y2=rnorm(20, reaction_run@solution$B, sd=true.sd2)
)
@

<<>>==
head(reaction_data)
@

Here, \code{y1} meausres quantity $A$ and \code{y2} measures quantity $B$.
Then, we can define the model:

<<>>===
reaction_model <- new("model.ode",
    name = "reaction",
    model = list(
        A ~ - k * A,
        B ~ 3 * k * A
    ),
    observation = list(
        y1 ~ dnorm(mean=A, sd=sd1),
        y2 ~ dnorm(mean=B, sd=sd2)
    ),
    initial = list(
        A~A0,
        B~B0
    ),
    par=c("k", "A0", "B0", "sd1", "sd2")
)
@

We can fit this using arbitrary starting conditions.

<<>>==
reaction_fit <- fitode(
    reaction_model,
    reaction_data,
    start=c(k=0.2, A0=300, B0=10, sd1=1, sd2=1)
)

plot(reaction_fit)
@

Note that optimizer may go to wrong places if we start with bad starting conditions.
In that case, we can try starting at different values.
Here's how you can set up a simple Latin Hypercube Sampling algorithm.

<<>>====
ranges <- data.frame(
    k=c(0.01, 0.2),
    A0=c(300, 350),
    B0=c(90, 110),
    sd1=c(10,20),
    sd2=c(10,20)
)

startpar <- apply(ranges, 2, function(x) seq(from=x[1], to=x[2], length.out=5))
startpar[] <- apply(startpar, 2,sample)

reaction_fitlist <- apply(startpar, 1, function(x){
    suppressMessages(fitode(
        reaction_model,
        reaction_data,
        start=x
    ))
})
@

We obtain a better fit:

<<>>===
reaction_max <- reaction_fitlist[[which.max(sapply(reaction_fitlist, logLik))]]
plot(reaction_max, level=0.95)
@

Note that this is a confidence intervals of trajectories not prediction intervals.
Prediction intervals would be wider than this.
We can look at the estimated parameters and their confidence intervals:

<<>>===
confint(reaction_max)
@





\end{document}
