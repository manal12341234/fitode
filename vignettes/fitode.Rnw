%\VignetteEngine{knitr::knitr}
%\VignetteDepends{ggplot2}
%\VignetteDepends{plyr}
%\VignetteDepends{dplyr}
%\VignetteDepends{reshape2}
%\VignetteIndexEntry{Basic ODE model fitting}
\documentclass{article}
\title{Basic ODE fitting}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{hyperref}
\newcommand{\rzero}{{\cal R}_0}
\newcommand{\code}[1]{{\tt #1}}
\bibliographystyle{chicago}
\date{\today}
\begin{document}
\maketitle

<<opts,echo=FALSE>>=
library("knitr")
opts_chunk$set(fig.width=6,fig.height=4)
@

\tableofcontents

\section{Preliminaries}

Load packages:

<<pkgs,message=FALSE>>=
library(fitode)
@

\section{Basic fitting}

\subsection{Exponential decay model}

Suppose we have a quantity that is decreasing exponentially.

<<>>==
set.seed(123)
true.m <- 0.5
true.A0 <- 200
true.sd <- 15

exp.data <- data.frame(
    x=rep(0:4, 3),
    y=rnorm(15, true.A0 * exp(-true.m * rep(0:4, 3)), sd=true.sd)
)

plot(exp.data)
@

The true dynamics can be modeled with the following equation:
$$
\frac{dA}{dt} = - m A
$$

We can translate this into a \code{fitode} model as follows:

<<>>==
exp.model <- new("model.ode",
    name = "SI",
    model = list(
        A ~ -m * A
    ),
    observation = list(
        y ~ dnorm(mean=A, sd=sd)
    ),
    initial = list(
        A ~ A0
    ),
    par=c("m", "A0", "sd")
)
@

Then, we can fit the model:

<<>>===
exp.fit <- fitode(
    exp.model,
    exp.data,
    start=c(m=0.5, A0=200, sd=15),
    tcol="x"
)
@

To diagnose the fit, we can use \code{plot} function.
Using the \code{level} argument will plot 95\% confidence intervals of the true trajectory, estimated via delta method.

<<>>==
plot(exp.fit, level=0.95)
curve(true.A0 * exp(-true.m*x), add=TRUE, lty=1, col="red")
legend(
    x="topright",
    legend=c("estimated", "true"),
    col=c("black", "red"),
    lty=1
)
@

To obtain the confidence interval, we can use \code{confint} function.
There are three available methods for obtaining the confidence intervals: \code{delta}, \code{profile} and \code{wmvrnorm}.
Due to computation speed, default option is \code{delta}.
We will get into the details later.
For now,

<<>>==
confint(exp.fit)
@

Note that in this particular example, we can use \code{glm} function to fit the model as well.
We can compare the results.

<<>>==
glm.fit <- glm(y~x,
    family=gaussian(link="log"),
    data=exp.data,
    start = c(intercept=log(200), x=-0.5))

glm.pred <- predict(glm.fit, data.frame(x=0:4), se.fit=TRUE, type="response")

glm.data <- data.frame(
    x=0:4,
    estimate=glm.pred$fit,
    lwr=glm.pred$fit-1.96 * glm.pred$se.fit,
    upr=glm.pred$fit+1.96 * glm.pred$se.fit
)

plot(exp.fit, level=0.95)
lines(glm.data$x, glm.data$estimate, col=2)
lines(glm.data$x, glm.data$lwr, col=2)
lines(glm.data$x, glm.data$upr, col=2)
legend(
    x="topright",
    legend=c("estimated (fitode)", "estimated (glm)"),
    col=c("black", "red"),
    lty=1
)
@

Estimated trajectories and their confidence intervals are essentially indistinguishable.

\subsection{Chemical reaction - multiple state fitting}

Now, consider the following chemical reaction:
$$
A \to 3 B
$$
Then, we can write the governing differential equation as follows:
$$
\begin{aligned}
\frac{dA}{dt} &= - k A\\
\frac{dB}{dt} &= 3 k B
\end{aligned}
$$
Suppose we have measured both quantities and have data:
<<echo=FALSE>>==
reaction_model <- new("model.ode",
    name = "reaction",
    model = list(
        A ~ - k * A,
        B ~ 3 * k * A
    ),
    observation = list(
        y1 ~ dnorm(mean=A, sd=sd),
        y2 ~ dnorm(mean=B, sd=sd)
    ),
    initial = list(
        A~A0,
        B~B0
    ),
    par=c("k", "A0", "B0", "sd")
)

true.k <- 0.1
true.A0 <- 300
true.B0 <- 100
true.sd <- 10

times <- 1:20

reaction_run <- ode.solve(reaction_model, times, c(k=true.k, A0=true.A0, B0=true.B0, sd1=true.sd))
reaction_data <- data.frame(
    times=times,
    y1=rnorm(20, reaction_run@solution$A, sd=true.sd),
    y2=rnorm(20, reaction_run@solution$B, sd=true.sd)
)
@

<<>>==
head(reaction_data)
@

Here, \code{y1} meausres quantity $A$ and \code{y2} measures quantity $B$.
Then, we can define the model:

<<>>===
reaction_model <- new("model.ode",
    name = "reaction",
    model = list(
        A ~ - k * A,
        B ~ 3 * k * A
    ),
    observation = list(
        y1 ~ dnorm(mean=A, sd=sd),
        y2 ~ dnorm(mean=B, sd=sd)
    ),
    initial = list(
        A~A0,
        B~B0
    ),
    par=c("k", "A0", "B0", "sd")
)
@

We can fit this using arbitrary starting conditions.

<<>>==
reaction_fit <- fitode(
    reaction_model,
    reaction_data,
    start=c(k=0.1, A0=300, B0=10, sd=10)
)

plot(reaction_fit, level=0.95)
@

Confidence intervals...

<<>>===
confint(reaction_fit)
@

\subsection{Fitting SIR model}

<<>>==
harbin <- fitsir::harbin
harbin2 <- rbind(data.frame(week=1, Deaths=NA), harbin)

plot(harbin2)
@

We need to add NA observation to make this work...

<<sirfit, warning=FALSE>>==
SI_model_c <- new("model.ode",
    name = "SI",
    model = list(
        S ~ - beta*S*I/N,
        I ~ beta*S*I/N - gamma*I,
        cDeath ~ gamma*I
    ),
    observation = list(
        Deaths ~ dnbinom(mu=cDeath, size=size)
    ),
    initial = list(
        S ~ N * (1 - i0),
        I ~ N * i0,
        cDeath ~ 0
    ),
    diffnames="cDeath",
    par=c("beta", "gamma", "N", "i0", "size")
)

start <- c(beta=2, gamma=1, N=20000, i0=1e-5, size=10)

sirfit <- fitode(
    SI_model_c,
    harbin2,
    start=start,
    link = list(
        beta="log",
        gamma="log",
        N="log",
        i0="logit",
        size="log"
    ),
    tcol="week"
)

plot(sirfit, level=0.95)
@

Confidence intervals on various epidemiological quantities:

<<>>==
confint(sirfit,
        parm=list(
            R0~beta/gamma,
            r~beta-gamma
        ),
        method="wmvrnorm")
@

Alternate parameterization:

<<sirfit2, warning=FALSE>>==
SI_model_c2 <- Transform(
    SI_model_c,
    list(
        beta~(R0_1+1)*gamma
    ),
    par=c("R0_1", "gamma", "N", "i0", "size")
)

cc <- coef(sirfit)

start2 <- c(R0_1=unname(cc[1]/cc[2]-1), cc[-1])

sirfit2 <- fitode(
    SI_model_c2,
    harbin2,
    start=start2,
    link = list(
        R0_1="log",
        gamma="log",
        N="log",
        i0="logit",
        size="log"
    ),
    tcol="week"
)

@

Compare fits:

<<>>==
plot(sirfit, level=0.95)
plot(sirfit2, level=0.95, add=TRUE, col.traj="red", col.conf="red")
@

We get identical fits. The advantage of this parameterization is that we can obtain profile confidence intervals on R0 (it's a little slow...):

<<sirconf, warning=FALSE>>==
set.seed(101)
confint(sirfit2, "R0_1", method="profile") + 1
confint(sirfit2, "R0_1", method="wmvrnorm") + 1
confint(sirfit, parm=list(R0~beta/gamma))
confint(sirfit, parm=list(R0~beta/gamma), method="wmvrnorm")
@

I'm not sure why performing wmvrnorm on beta gamma scale gives narrower confidence intervals. Maybe it's because of the parameterization?

<<>>==
set.seed(101)
plot(sirfit, level=0.95, method="wmvrnorm")
plot(sirfit2, level=0.95, add=TRUE, col.traj="red", col.conf="red", method="wmvrnorm")
@

\end{document}
